{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"utils.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"MkNx8ovAHSHz"},"source":["train_path = '/content/gdrive/MyDrive/deep_learning_project/numpy data'\n","initial_generator_path = '/content/gdrive/MyDrive/deep_learning_project/models/gan/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9Dt_gwjuZ3x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619886354955,"user_tz":-120,"elapsed":3923,"user":{"displayName":"MATTEO DESSÃŒ","photoUrl":"","userId":"04106704574472318930"}},"outputId":"d848cddd-d0cd-46dc-d0cc-f0b783abf7a7"},"source":["import numpy as np\n","import os \n","import matplotlib.pyplot as plt\n","import shutil\n","from PIL import Image \n","import PIL\n","import cv2 as cv\n","from sklearn.metrics import classification_report,confusion_matrix,precision_score,recall_score,roc_curve,roc_curve,fbeta_score,auc\n","from imblearn.over_sampling import SMOTE\n","import random\n","import tensorflow as tf\n","from tensorflow.keras import models\n","from mlxtend.plotting import plot_confusion_matrix\n","\n","def enhance_contrast(images,contrast=1.5):\n","#def enhance_contrast(images,contrast=2):\n","  samples_expanded = np.expand_dims(images, -1)\n","  #print(np.shape(samples_expanded))\n","  temp = []\n","  for elem in samples_expanded:\n","    temp.append(tf.image.adjust_contrast(elem, contrast))\n","\n","  temp=np.squeeze(temp, axis=-1)\n","  samples_adjusted=np.array(temp)\n","  return samples_adjusted\n","\n","def enhance_image(imgs):\n","  \n","  enhanced_images = []\n","  clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","  for img in imgs:\n","    enhanced_images.append(clahe.apply(img))\n","  return enhanced_images\n","\n","def filter_dataset(dataset):\n","  filtered_images = []\n","  for img in dataset:\n","    info = np.iinfo(img.dtype) # Get the information of the incoming image type\n","    data = img.astype(np.float64) / info.max # normalize the data to 0 - 1\n","    data = 255 * data # Now scale by 255\n","    img = data.astype(np.uint8)\n","    ret, thresh1 = cv.threshold(img, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU  )\n","    thresh1 = thresh1.astype(np.float64)\n","    thresh1 = thresh1 / 255\n","    thresh1 = thresh1 * 65536\n","    filtered_images.append(thresh1)\n","  return filtered_images\n","\n","def gan_oversample(images,labels,traduction):\n","\n","  traduction_path = \"\"\n","\n","  if(traduction==0):\n","    traduction_path = \"benign_malign/\"\n","  else:\n","    traduction_path = \"mass_calcification/\"\n","\n","  indexes_0 = []\n","  indexes_1 = []\n","\n","  for i in range(len(images)):\n","    if(labels[i]==0):\n","      indexes_0.append(i)\n","    else:\n","      indexes_1.append(i)\n","  \n","  difference = (len(indexes_0)-len(indexes_1))\n","\n","  adding_images = []\n","  adding_labels = []\n","  oversampling_label = 0\n","  \n","  if(difference>0):\n","    print(\"Oversampling on label 1\")\n","    total_path = (initial_generator_path + traduction_path +'1/generator.h5')\n","    oversampling_label = 1\n","  else:\n","    print(\"Oversampling on label 0\")\n","    total_path = (initial_generator_path + traduction_path + '0/generator.h5')\n","\n","  generator = models.load_model(total_path)\n","\n","  while(len(adding_images)<np.abs(difference)):\n","    noise= np.random.normal(loc=0, scale=1, size=[100, 100])\n","    generated_images = generator.predict(noise)\n","    generated_images = generated_images * 65536\n","    generated_images = generated_images.reshape(100,150,150)\n","    for img in generated_images:\n","      if(len(adding_images)<np.abs(difference)):\n","        adding_images.append(np.uint16(img))\n","    \n","  #print(np.shape(adding_images))\n","    \n","  if(oversampling_label==1):\n","    adding_labels = np.ones(len(adding_images))\n","  else:\n","    adding_labels = np.zeros(len(adding_images))\n","  \n","  concatenated_images = np.concatenate((images,adding_images),axis=0)\n","  concatenated_labels = np.concatenate((labels,adding_labels),axis=0)\n","\n","  return concatenated_images,concatenated_labels\n","\n","def in_depth_performance(testing_labels,prediction):\n","\n","  #print(\"confusion matrix: \\n\", confusion_matrix(testing_labels,prediction))\n","  fig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(testing_labels,prediction), figsize=(6, 6), cmap=plt.cm.Blues)\n","  plt.xlabel('Predicted label', fontsize=18)\n","  plt.ylabel('True label', fontsize=18)\n","  plt.title('Confusion Matrix', fontsize=18)\n","  classNames = ['0','1']\n","  tick_marks = np.arange(len(classNames))\n","  plt.xticks(tick_marks, classNames)\n","  plt.yticks(tick_marks, classNames)  \n","  plt.show()\n","  print(\"precision score: \", precision_score(testing_labels,prediction))\n","  print(\"recall score: \", recall_score(testing_labels,prediction))\n","  print(\"f2: \", fbeta_score(testing_labels, prediction,2))\n","  print(\"f0.5: \", fbeta_score(testing_labels, prediction,0.5))\n","  print(\"classification report: \\n\", classification_report(testing_labels,prediction))\n","  fpr, tpr, _ = roc_curve(testing_labels,  prediction)\n","  print(\"AUC: \",auc(fpr,tpr))\n","  plt.plot(fpr,tpr)\n","  plt.ylabel('True Positive Rate')\n","  plt.xlabel('False Positive Rate')\n","  plt.title('Roc curve')\n","  plt.show()\n","\n","def labels_distribution(labels,title):\n","\n","  print(\"------------------------------------------------\")\n","  print(title)\n","  zero_count = np.count_nonzero( labels == 0)\n","  one_count = np.count_nonzero( labels == 1)\n","  two_count = np.count_nonzero( labels == 2)\n","  three_count = np.count_nonzero( labels == 3)\n","  four_count = np.count_nonzero( labels == 4)\n","  y = (zero_count,one_count,two_count,three_count,four_count)\n","  x = (0,1,2,3,4)\n","  plt.bar(x,y,align='center') # A bar chart\n","  plt.xlabel('Class')\n","  plt.ylabel('Occurance')\n","  for i in range(len(y)):\n","    plt.vlines(x[i],0,y[i]) # Here you are drawing the horizontal lines\n","  plt.show()\n","\n","def label_traduction(labels,type):\n","  traducted_labels = []\n","  if(type==0):\n","    traducted_labels =  np.array(labels%2, dtype='int')\n","  else:\n","    traducted_labels =  np.array(labels/3, dtype='int')\n","  return traducted_labels\n","\n","def load_testing():\n","\n","  images = np.load(os.path.join(train_path,'public_test_tensor.npy'))\n","  labels = np.load(os.path.join(train_path,'public_test_labels.npy'))\n","\n","  images = images[1::2]\n","  labels = labels[1::2]\n","\n","  return images,labels\n","\n","def load_training():\n","\n","  images = np.load(os.path.join(train_path,'train_tensor.npy'))\n","  labels = np.load(os.path.join(train_path,'train_labels.npy'))\n","\n","  images = images[1::2]\n","  labels = labels[1::2]\n","\n","  return images,labels\n","\n","def oversample(images,labels):\n","\n","  indexes_0 = []\n","  indexes_1 = []\n","\n","  for i in range(len(images)):\n","    if(labels[i]==0):\n","      indexes_0.append(i)\n","    else:\n","      indexes_1.append(i)\n","  \n","  difference = (len(indexes_0)-len(indexes_1))\n","\n","  adding_images = []\n","  adding_labels = []\n","  \n","  if(difference>0):\n","    print(\"Oversampling on label 1\")\n","    random_indexes = np.random.choice(indexes_1,difference,replace=True)\n","    for index in random_indexes:\n","      adding_images.append(images[index])\n","      adding_labels.append(labels[index])\n","  else:\n","    print(\"Oversampling on label 0\")\n","    random_indexes = np.random.choice(indexes_0,abs(difference),replace=True)\n","    for index in random_indexes:\n","      adding_images.append(images[index])\n","      adding_labels.append(labels[index])\n","  \n","  concatenated_images = np.concatenate((images,adding_images),axis=0)\n","  concatenated_labels = np.concatenate((labels,adding_labels),axis=0)\n","\n","  return concatenated_images,concatenated_labels\n","\n","\n","def save_dataset(path, dataset_type, images, labels):\n","  print(\"------------------------------------------------\")\n","  print(\"Creating folders...\")\n","  num_labels = len( np.unique(labels) )\n","  \n","  try:\n","    shutil.rmtree(path + dataset_type)\n","  except OSError as e:\n","    print(\"Warn: %s - %s.\" % (e.filename, e.strerror))\n","\n","  os.mkdir(path + dataset_type )\n","\n","  for i in range( num_labels ): \n","    os.mkdir( path + dataset_type + \"/\" + str(i) )\n","  print(\"Saving images...\")\n","  for i in range( len(images) ):\n","    # creating a image object (main image) \n","    im1 = Image.fromarray( images[i] )\n","    im1 = enhance_image(im1)\n","    im1 = im1.save( path + dataset_type + \"/\" + str(labels[i]) + \"/\" + str(i) + \".png\")\n","  print(\"DONE!\")\n","  print(\"------------------------------------------------\")\n","\n","def shuffle_data(images,labels):\n","\n","  indexes = list(range(len(images)))\n","  random.shuffle(indexes)\n","\n","  reordered_images = images\n","  reordered_labels = labels\n","\n","  for i in indexes:\n","\n","    reordered_images[i] = images[i]\n","    reordered_labels[i] = labels[i]\n","  \n","  return reordered_images,reordered_labels\n","\n","def smote_oversample(images,labels):\n","  reshaped_data = images.reshape(-1,images.shape[0])\n","  oversampled_images,oversampled_labels = SMOTE().fit_resample(images,labels)\n","  return oversampled_images.reshape(np.roll(oversampled_images.shape,0)),oversampled_labels\n","\n","def visualize_image(data,title):\n"," \n","  plt.imshow(data,cmap='gray')\n","  plt.title(title)\n","  plt.axis(\"off\")\n","  plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]}]}